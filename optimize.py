import optuna
import argparse
from argparse import Namespace
import torch
import numpy as np

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from config import config
from core.managers.experiment import ExperimentManager
from core.managers.runner import train


def objective(trial):
    # ==========================================
    # 1. [í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì²œ ë°›ê¸°]
    # ==========================================

    # í•™ìŠµë¥  (Learning Rate): ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ íƒìƒ‰ (0.00001 ~ 0.001)
    lr = trial.suggest_float("lr", 1e-5, 1e-3, log=True)

    # ê°ë§ˆ (Gamma): ë¯¸ë˜ ë³´ìƒ ì¤‘ìš”ë„ (0.9 ~ 0.999)
    gamma = trial.suggest_float("gamma", 0.90, 0.999)

    # ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ (Entropy Coef): íƒí—˜ ë¹„ìœ¨ (0.01 ~ 0.1)
    entropy_coef = trial.suggest_float("entropy_coef", 0.01, 0.1)

    # ë°°ì¹˜ í¬ê¸° (Batch Size): 64, 128, 256 ì¤‘ ì„ íƒ
    batch_size = trial.suggest_categorical("batch_size", [128, 256])

    # ì‹ ê²½ë§ í¬ê¸° (Hidden Dim): ëª¨ë¸ ë³µì¡ë„
    hidden_dim = trial.suggest_categorical("hidden_dim", [64, 128, 256])

    print(
        f"\n[Trial {trial.number}] Testing: LR={lr:.5f}, Gamma={gamma:.3f}, Batch={batch_size}, Hidden={hidden_dim}"
    )

    # ==========================================
    # 2. [ì„¤ì • ë®ì–´ì“°ê¸°]
    # ==========================================
    # config.pyì˜ ì „ì—­ ì„¤ì •ê°’ì„ Optunaê°€ ì¶”ì²œí•œ ê°’ìœ¼ë¡œ ì„ì‹œ êµì²´
    config.train.LR = lr
    config.train.GAMMA = gamma
    config.train.ENTROPY_COEF = entropy_coef
    config.train.BATCH_SIZE = batch_size

    # ==========================================
    # 3. [ì‹¤í—˜ í™˜ê²½ êµ¬ì¶•]
    # ==========================================
    # ExperimentManagerë¥¼ ìœ„í•œ ê°€ì§œ ì¸ì(Namespace) ìƒì„±
    # ìµœì í™” ë•ŒëŠ” ì—í”¼ì†Œë“œë¥¼ ì ê²Œ(ì˜ˆ: 300íŒ) ì„¤ì •í•´ì„œ ë¹ ë¥´ê²Œ í›‘ì–´ë³´ëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤.
    args = Namespace(
        mode="train",
        episodes=300,  # 300íŒë§Œ ëŒë ¤ë³´ê³  íŒë‹¨ (ë„ˆë¬´ ê¸¸ë©´ ì˜¤ë˜ ê±¸ë¦¼)
        player_configs=[
            # 8ëª… ëª¨ë‘ PPO RLAgentë¡œ ì„¤ì • (ëª¨ë¸ í¬ê¸°ëŠ” ìœ„ì—ì„œ ì¶”ì²œë°›ì€ ê°’ ì ìš©)
            {
                "type": "rl",
                "algo": "ppo",
                "backbone": "lstm",
                "hidden_dim": hidden_dim,
                "num_layers": 2,
                "role": "random",  # ëœë¤ ì—­í•  ë°°ì •
            }
            for _ in range(8)
        ],
        # ë¡œê·¸ ê²½ë¡œëŠ” trial ë²ˆí˜¸ë³„ë¡œ ë¶„ë¦¬
        paths={"log_dir": f"logs/optuna/trial_{trial.number}", "model_dir": "models"},
    )

    experiment = ExperimentManager(args)
    final_score = -999.0

    try:
        # í™˜ê²½ ìƒì„± (Optuna ì¤‘ì—ëŠ” num_cpus=0 ë˜ëŠ” 1ë¡œ ì„¤ì •í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì‹¤í–‰)
        # ì´ë¯¸ Optuna ìì²´ê°€ ë¬´ê±°ìš°ë¯€ë¡œ ë³‘ë ¬ í™˜ê²½ì„ ê³¼í•˜ê²Œ ì“°ë©´ ë©ˆì¶œ ìˆ˜ ìˆìŒ
        env = experiment.build_vec_env(
            num_envs=8, num_cpus=0
        )  # 0 = ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰

        agents = experiment.build_agents()
        rl_agents = experiment.get_rl_agents(agents)

        # ==========================================
        # 4. [í•™ìŠµ ì‹¤í–‰]
        # ==========================================
        # runner.pyì˜ train í•¨ìˆ˜ê°€ 'ë§ˆì§€ë§‰ í‰ê·  ë³´ìƒ'ì„ ë°˜í™˜í•´ì•¼ í•¨
        final_score = train(
            env=env,
            rl_agents=rl_agents,
            all_agents=agents,
            args=args,
            logger=experiment.logger,
        )

        # NaN(ìˆ˜ì¹˜ ì˜¤ë¥˜)ì´ ë‚˜ì˜¤ë©´ ìµœí•˜ì  ì²˜ë¦¬
        if np.isnan(final_score):
            final_score = -999.0

    except Exception as e:
        print(f"[Optuna Error] Trial {trial.number} failed: {e}")
        final_score = -999.0  # ì—ëŸ¬ ë‚˜ë©´ ìµœí•˜ì 

    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (íŒŒì¼ ë‹«ê¸° ë“±)
        experiment.close()
        try:
            env.close()
        except:
            pass

    # Optunaì—ê²Œ ì ìˆ˜ ë³´ê³ 
    return final_score


if __name__ == "__main__":
    # 1. DB íŒŒì¼ì— ì €ì¥ (ì¤‘ê°„ì— êº¼ì ¸ë„ ì´ì–´í•˜ê¸° ê°€ëŠ¥)
    storage_name = "sqlite:///mafia_optuna.db"

    # 2. ìŠ¤í„°ë”” ìƒì„± (Maximize: ë³´ìƒì„ ë†’ì´ëŠ” ê²Œ ëª©í‘œ)
    # Pruner: ì´ˆë°˜ 50íŒ(warmup)ì€ ë´ì£¼ê³ , ê·¸ ë’¤ë¡œ í•˜ìœ„ 50%ëŠ” ê°€ì°¨ ì—†ì´ ìë¦„
    study = optuna.create_study(
        study_name="mafia-ppo-tuning",
        direction="maximize",
        storage=storage_name,
        load_if_exists=True,
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=50),
    )

    print("=== ğŸ•µï¸ Mafia AI Hyperparameter Optimization Start ===")
    print(f"Logs will be saved to: {storage_name}")

    # 3. ìµœì í™” ì‹¤í–‰ (20ë²ˆ ì‹œë„)
    study.optimize(objective, n_trials=1)

    # 4. ê²°ê³¼ ì¶œë ¥
    print("\n==================================")
    print(f"ğŸ† Best Value (Reward): {study.best_value}")
    print(f"ğŸ† Best Params: {study.best_params}")
    print("==================================")

    # ëŒ€ì‹œë³´ë“œ ì‹¤í–‰ ëª…ë ¹ì–´ ì•ˆë‚´
    print(f"\n[Tip] View dashboard: optuna-dashboard {storage_name}")
